{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_md = pd.read_csv('titanic_MD.csv')\n",
    "titanic = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic.shape)\n",
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fill_nas:\n",
    "    def __init__(self, df):\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        self.df = df\n",
    "        self.df.replace(r'\\?', np.nan, regex = True, inplace = True)\n",
    "        self.filled_df = self.df.copy()\n",
    "        self.cols_missing_object = []\n",
    "        self.cols_missing_float = []\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object' and self.df[col].isna().sum() > 0:\n",
    "                self.cols_missing_object.append(col)\n",
    "            elif self.df[col].dtype == 'float' and self.df[col].isna().sum() > 0:\n",
    "                self.cols_missing_float.append(col)\n",
    "    \n",
    "    def missing_cases(self):\n",
    "        return self.df.isna().sum()\n",
    "    \n",
    "    def missing_percentage(self):\n",
    "        return self.df.isna().sum()/self.df.shape[0]\n",
    "    \n",
    "    def missing_objects(self):\n",
    "        return self.cols_missing_object\n",
    "    \n",
    "    def missing_floats(self):\n",
    "        return self.cols_missing_float\n",
    "    \n",
    "    def listwise_deletion(self, col = 'All'):\n",
    "        if col == 'All':\n",
    "            return self.filled_df.dropna()\n",
    "        elif type(col) is int: \n",
    "            return self.filled_df.iloc[:,col:col+1].dropna()\n",
    "        elif type(col) is str:\n",
    "            return self.filled_df[[col]].dropna()\n",
    "    \n",
    "    def pairwise_deletion(self, col_1 = 0, col_2 = 1):\n",
    "        column_1 = self.df.columns[col_1]\n",
    "        column_2 = self.df.columns[col_2]\n",
    "        db = self.filled_df[self.filled_df[column_1].notna() & self.filled_df[column_2].notna()]\n",
    "        return db\n",
    "    \n",
    "    def imputation(self, col = 'Sex', method = 'mean'):\n",
    "        if self.filled_df[col].dtype == 'object':\n",
    "            imp = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "            db = self.filled_df[[col]]\n",
    "            return imp.fit_transform(db)\n",
    "        elif self.filled_df[col].dtype == 'float':\n",
    "            imp = SimpleImputer(missing_values = np.nan, strategy = method)\n",
    "            db = self.filled_df[[col]]\n",
    "            return imp.fit_transform(db)\n",
    "        \n",
    "    def sectorized_imputation(self, col = 'Sex'):\n",
    "        if col == 'Sex' or col == 4:\n",
    "            row_female_missing = self.filled_df[col].isna() & self.filled_df['Name'].str.contains(pat = '(Mlle|Miss|Mrs|Countess|Mme|Lady)\\.',regex = True)\n",
    "            row_male_missing = self.filled_df[col].isna() & self.filled_df['Name'].str.contains(pat = '(Mr|Capt|Master|Dr|Col|Major|Sir)\\.',regex = True)\n",
    "            self.filled_df[col] = np.where(row_female_missing == True, 'female', self.filled_df[col])\n",
    "            self.filled_df[col] = np.where(row_male_missing == True, 'male', self.filled_df[col])\n",
    "            db = self.filled_df[[col]]\n",
    "            return db\n",
    "        elif col not in ['Sex','Survived','Pclass'] or col not in [1,2,4] and col in self.df.columns:\n",
    "            if self.filled_df[col].dtype == 'object':\n",
    "                dic_values = pd.DataFrame(self.filled_df.groupby(['Survived', 'Pclass'])[col].agg(lambda x:x.value_counts().index[0])).reset_index()\n",
    "                dic_values.columns = ['Survived', 'Pclass', 'Col']\n",
    "                df = self.filled_df.merge(dic_values, how = 'left', on = ['Survived', 'Pclass'])\n",
    "                df[col+'_new'] = np.where(df[col].isna(), df['Col'], df[col])\n",
    "                return df[[col+'_new']]\n",
    "            elif self.filled_df[col].dtype == 'float':\n",
    "                dic_values = pd.DataFrame(self.filled_df.groupby(['Survived', 'Pclass'])[col].agg(lambda x:x.median())).reset_index()\n",
    "                dic_values.columns = ['Survived', 'Pclass', 'Col']\n",
    "                df = self.filled_df.merge(dic_values, how = 'left', on = ['Survived', 'Pclass'])\n",
    "                df[col+'_new'] = np.where(df[col].isna(), df['Col'], df[col])\n",
    "                return df[[col+'_new']]\n",
    "    \n",
    "    def predictive_model(self, col):\n",
    "        if self.filled_df[col].dtype == 'object' and col in self.cols_missing_object:\n",
    "            df = self.filled_df[['Survived','Pclass',col]].copy()\n",
    "            df = df.dropna()\n",
    "            if col == 'Sex':\n",
    "                df['Sex'] = np.where(df['Sex'] == 'male',1,0)\n",
    "            elif col == 'Embarked':\n",
    "                df['Embarked'] = np.where(df['Embarked'] == 'S',0,df['Embarked'])\n",
    "                df['Embarked'] = np.where(df['Embarked'] == 'C',1,df['Embarked'])\n",
    "                df['Embarked'] = np.where(df['Embarked'] == 'Q',2,df['Embarked'])\n",
    "            lr = LogisticRegression(multi_class='multinomial', solver = 'newton-cg')\n",
    "            x_train = df[['Survived','Pclass']]\n",
    "            y_train = df[[col]]\n",
    "            lr.fit(x_train,y_train.astype('int'))\n",
    "            y_pred = lr.predict(self.filled_df[['Survived','Pclass']])\n",
    "            if col == 'Sex':\n",
    "                y_pred = np.where(y_pred == 0,'female','male')\n",
    "            elif col == 'Embarked':\n",
    "                y_pred = np.where(y_pred == 0,'S',y_pred)\n",
    "                y_pred = np.where(y_pred == 1,'C',y_pred)\n",
    "                y_pred = np.where(y_pred == 2,'Q',y_pred)\n",
    "            return y_pred\n",
    "        elif self.filled_df[col].dtype == 'float' and col in self.cols_missing_float:\n",
    "            df = self.filled_df[['Survived','Pclass',col]].copy()\n",
    "            df = df.dropna()\n",
    "            lm = LinearRegression()\n",
    "            x_train = df[['Survived','Pclass']]\n",
    "            y_train = df[[col]]\n",
    "            lm.fit(x_train,y_train)\n",
    "            y_pred = lr.predict(self.filled_df[['Survived','Pclass']])\n",
    "            return y_pred\n",
    "    \n",
    "    def outlier_sda(self, col, sigma = 3, option = 'cap'):\n",
    "        if col in self.cols_missing_float:\n",
    "            avrg = np.nanmean(self.filled_df[col])\n",
    "            sdev = np.nanstd(self.filled_df[col])\n",
    "            upper_lim = avrg + sdev * sigma\n",
    "            lower_lim = avrg - sdev * sigma\n",
    "            if option == 'cap':\n",
    "                capped_res = np.where(self.filled_df[col]<lower_lim,lower_lim,np.where(self.filled_df[col]>upper_lim,upper_lim,self.filled_df[col]))\n",
    "                return capped_res\n",
    "            else:\n",
    "                deleted_res = self.filled_df[(self.filled_df[col]>=lower_lim) & (self.filled_df[col]<=upper_lim)][[col]]\n",
    "                return deleted_res\n",
    "            \n",
    "    def outlier_pa(self, col, percentile_width = 5, option = 'cap'):\n",
    "        if col in self.cols_missing_float:\n",
    "            lower_lim = np.percentile(self.filled_df[col], percentile_width)\n",
    "            upper_lim = np.percentile(self.filled_df[col], 100 - percentile_width)\n",
    "            if option == 'cap':\n",
    "                capped_res = np.where(self.filled_df[col]<lower_lim,lower_lim,np.where(self.filled_df[col]>upper_lim,upper_lim,self.filled_df[col]))\n",
    "                return capped_res\n",
    "            else:\n",
    "                deleted_res = self.filled_df[(self.filled_df[col]>=lower_lim) & (self.filled_df[col]<=upper_lim)][[col]]\n",
    "                return deleted_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normalize_data:\n",
    "    def __init__(self, df):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        from sklearn.preprocessing import MaxAbsScaler\n",
    "        self.df = df\n",
    "        self.number_cols = self.df.select_dtypes(include=['float', 'int']).columns\n",
    "    \n",
    "    def norm_strat(strategy):\n",
    "        df_normalized = self.df.copy()\n",
    "        if strategy == 'standard':\n",
    "            scaler1 = StandardScaler()\n",
    "            for col in self.number_cols:\n",
    "                df_normalized[col+'_z'] = scaler1.fit_transform(df_normalized[[col]])\n",
    "            return df_normalized\n",
    "        elif strategy == 'minmax':\n",
    "            scaler2 = MinMaxScaler()\n",
    "            for col in self.number_cols:\n",
    "                df_normalized[col+'_mm'] = scaler2.fit_transform(df_normalized[[col]])\n",
    "            return df_normalized\n",
    "        elif strategy == 'maxabs':\n",
    "            scaler3 = MaxAbsScaler()\n",
    "            for col in self.number_cols:\n",
    "                df_normalized[col+'_ma'] = scaler3.fit_transform(df_normalized[[col]])\n",
    "            return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing = fill_nas(titanic_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Reporte detallado de missing data para todas las columnas.\n",
    "Como se puede ver a continuación, existen datos faltantes en las columnas de Sex, Age, SibSp, Parch, Fare y Embarked. Las columnas previamente mencionadas son en su mayoria columnas numericas, ya que unicamente las columnas de Sex y Embarked poseen datos textuales. De todas las columnas mencionadas anteriormente, las columnas que mas faltantes tienen son las de Age y Sex, con el 27.86% y el 13.66% de los datos totales respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.missing_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.missing_percentage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.missing_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.missing_floats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Para cada columna especificar que tipo de modelo se utilizará y qué valores se le darán a todos los missing values.\n",
    "'Sex' = Sectorized imputation through 'Name' column, based on title.\n",
    "\n",
    "'Embarked' = Predictive modelling using a logistic regressor based on 'Survived' and 'Pclass', used these columns since they were the only ones with complete cases.\n",
    "\n",
    "'Age' = Sectorized imputation through 'Survived' and 'Pclass' columns using the median for each category as a result.\n",
    "\n",
    "'SibSp' = Regular imputation using the median as a basis.\n",
    "\n",
    "'Parch' = Regular imputation using the median as a basis.\n",
    "\n",
    "'Fare' = Predictive modelling using a linear regressor based on 'Survived' and 'Pclass', used these columns since they were the only ones with complete cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Reporte de qué filas están completas\n",
    "Como se podra ver en la línea inferior, al momento de eliminar todas las filas que no tienen datos completos, la longitud del set de datos se reduce a 100 filas. Esto nos indica que, al evaluar contra el tamaño original del archivo, los datos originales tienen 83 casos incompletos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_md.shape[0] - db_missing.listwise_deletion('All').shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Utilizar los siguientes métodos para cada columna que contiene missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.Listwise deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.listwise_deletion('All')['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.listwise_deletion('All')['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('All')['SibSp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('All')['Parch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('All')['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('All')['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.Pairwise deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.listwise_deletion('Age')['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.listwise_deletion('Fare')['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('SibSp')['SibSp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('Parch')['Parch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('Sex')['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(db_missing.listwise_deletion('Embarked')['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.Imputación General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Parch','mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Parch','most_frequent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Parch','median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('SibSp','mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('SibSp','most_frequent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('SibSp','median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Age','mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Age','median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Age','most_frequent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Fare','mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Fare','median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(db_missing.imputation('Fare','most_frequent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.imputation('Sex','most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.imputation('Embarked','most_frequent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Imputación Secotrizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.sectorized_imputation('Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.sectorized_imputation('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.sectorized_imputation('Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.sectorized_imputation('SibSp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.sectorized_imputation('Parch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.sectorized_imputation('Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Modelo de Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.predictive_model('Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.predictive_model('Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.predictive_model('Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.predictive_model('SibSp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.predictive_model('Parch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.predictive_model('Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Outliers:Standard Deviation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_sda('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_sda('SibSp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_sda('Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_sda('Parch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. Outliers: Percentile Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_pa('Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_pa('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_pa('SibSp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_missing.outlier_pa('Parch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Al comparar los métodos del inciso 4 contra “titanic.csv”, ¿Qué método (para cada columna) se acerca más a la realidad y por qué?\n",
    "Como podemos ver, el metodo de imputación sectorizado para determinar el campo de Sexo es muy bueno, fallando unicamente en un solo caso. Ademas de esto, podemos ver que para las columnas de 'Parch', 'SibSp' y 'Embarked', el metodo de Listwise deletion se acerca mas a la realidad, seguramente por el hecho de que no se pierden tantos datos. El método de mean tambien sirve como una buena imputación para la columan de fare, ya que muchos de los valores se encuentran agrupados en rangos más bajos y la falta de 8 datos no distorsiona mucho la distribución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones\n",
    "Podemos concluir que para este set de datos, existen metodos más exitosos que otros. Los modelos regresivos no parecen servir tan bien con una cantidad tan poca de datos, lo cual causa que sus resultados sean increiblemente distorsionados y poco efectivos, como se puede ver en los resultados de las regresiones. El metodo de eliminación pairwise permite que mantengamos mucho más de los datos, reduciendo el impacto total de las variables faltantes, al igaul que manteniendo la distribución de los datos como tal. Por último, cabe mencionar que las imputaciones sectorizadas son más efectivas al encontrar un patrón consistente en las variables a utilizar para realizarla. Esta efectividad se pudo ver en la imputación de la columna 'Sex', ya que se baso mucho en el nombre de la persona y en especifico, su titulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tLuego del pre-procesamiento de la data con Missing Values, normalice las columnas numéricas por los métodos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_complete = db_missing.listwise_deletion('All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = normalize_data(dataframe_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df.norm_strat('absmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df.norm_strat('minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df.norm_strat('standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Compare los estadísticos que considere más importantes para su conclusión y compare contra la data completa de “titanic.csv” (deberán de normalizar también). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = normalize_data(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.norm_strat('standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df.norm_strat('minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df.norm_strat('absmax')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
